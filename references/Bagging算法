
////Bagging算法：训练m个采样集，训练出的分类器无依存关系
随机采样，放回采样
对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是1m。不被采集到的概率为1−1m。如果m次采样都没有被采集中的概率是(1−1m)m。当m→∞时，(1−1m)m→1e≃0.368。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。
　　　　对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。




上一节我们对bagging算法的原理做了总结，这里就对bagging算法的流程做一个总结。相对于Boosting系列的Adaboost和GBDT，bagging算法要简单的多。

　　　　输入为样本集D={(x,y1),(x2,y2),...(xm,ym)}，弱学习器算法, 弱分类器迭代次数T。

　　　　输出为最终的强分类器f(x)
　　　　1）对于t=1,2...,T:

　　　　　　a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集Dt
　　　　　　b)用采样集Dt训练第t个弱学习器Gt(x)
　　　　2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。
